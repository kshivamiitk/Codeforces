{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disclaimer: Please act drowsy for the next 10 seconds.\n",
      "starting in 3\n",
      "starting in 2\n",
      "starting in 1\n",
      "Recording video for 10 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Disclaimer: Inform the user to act drowsy\n",
    "print(\"Disclaimer: Please act drowsy for the next 10 seconds.\")\n",
    "#setting the disclaimer to 10 seconnds to allow the user to act drowsy\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "time.sleep(1)\n",
    "print(\"starting in 3\")\n",
    "time.sleep(1)\n",
    "print(\"starting in 2\")\n",
    "time.sleep(1)\n",
    "print(\"starting in 1\")\n",
    "\n",
    "def capturing_videos_and_saving_to_frame(duration, output_folder):\n",
    "    \"\"\"\n",
    "    Capture video for a given duration and save frames as JPEG images.\n",
    "    \n",
    "    Parameters:\n",
    "        duration (int): Duration to capture video in seconds.\n",
    "        output_folder (str): Path to save the captured frames.\n",
    "    \"\"\"\n",
    "    # Create the output directory if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Open the default camera\n",
    "    cap = cv2.VideoCapture(1)\n",
    "\n",
    "    # Check if the camera opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open webcam.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Recording video for {duration} seconds...\")\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If frame is read correctly, ret is True\n",
    "        if not ret:\n",
    "            print(\"Error: Can't receive frame (stream end?). Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Save frame as JPEG file\n",
    "        frame_filename = os.path.join(output_folder, f'frame_{frame_count:04d}.jpg')\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Frame', frame)\n",
    "\n",
    "        # Stop capturing after the specified duration\n",
    "        if time.time() - start_time > duration:\n",
    "            break\n",
    "\n",
    "        # Break the loop on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the capture and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Record video of drowsy behavior (10 seconds)\n",
    "capturing_videos_and_saving_to_frame(10, 'drowsy_frames')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait for 5 seconds.\n",
      "Please act normal for the next 10 seconds.\n",
      "Recording video for 10 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Wait for 5 seconds\n",
    "print(\"Please wait for 5 seconds.\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Disclaimer: Inform the user to act normal\n",
    "print(\"Please act normal for the next 10 seconds.\")\n",
    "\n",
    "# Record video of normal behavior (10 seconds)\n",
    "capturing_videos_and_saving_to_frame(10, 'normal_frames')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la\n",
    "from scipy.spatial import distance\n",
    "import dlib\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to calculate Mouth Aspect Ratio (MAR)\n",
    "def calculate_mar(mouth):\n",
    "    A = distance.euclidean(mouth[3], mouth[9])\n",
    "    B = distance.euclidean(mouth[2], mouth[10])\n",
    "    C = distance.euclidean(mouth[4], mouth[8])\n",
    "    D = distance.euclidean(mouth[0], mouth[6])\n",
    "    mar = (A + B + C) / (3.0 * D)\n",
    "    return mar\n",
    "\n",
    "# Load the facial landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction code remains similar, reuse the EAR and MAR calculation functions\n",
    "# Extract features from 'drowsy_frames' and 'normal_frames' folders.\n",
    "# Save features as a .csv file for training later.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def extract_features_from_frames(input_folder, label, output_csv):\n",
    "    \"\"\"\n",
    "    Extract EAR, MAR, and other features from frames and save to CSV.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    files = sorted(glob.glob(os.path.join(input_folder, \"*.jpg\")))\n",
    "\n",
    "    for file in files:\n",
    "        frame = cv2.imread(file)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "\n",
    "        for face in faces:\n",
    "            landmarks = predictor(gray, face)\n",
    "            left_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)])\n",
    "            right_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)])\n",
    "            mouth = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(48, 68)])\n",
    "\n",
    "            ear = (calculate_ear(left_eye) + calculate_ear(right_eye)) / 2.0\n",
    "            mar = calculate_mar(mouth)\n",
    "            data.append([ear, mar, label])\n",
    "\n",
    "    # Save data to CSV\n",
    "    df = pd.DataFrame(data, columns=['EAR', 'MAR', 'Label'])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Extract features for both classes\n",
    "extract_features_from_frames('drowsy_frames', 1, 'drowsy_features.csv')\n",
    "extract_features_from_frames('normal_frames', 0, 'normal_features.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6932 - loss: 0.6232 - val_accuracy: 0.7712 - val_loss: 0.5533\n",
      "Epoch 2/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6983 - loss: 0.6074 - val_accuracy: 0.7966 - val_loss: 0.5139\n",
      "Epoch 3/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7770 - loss: 0.5527 - val_accuracy: 0.8136 - val_loss: 0.4714\n",
      "Epoch 4/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 871us/step - accuracy: 0.7552 - loss: 0.5202 - val_accuracy: 0.8220 - val_loss: 0.4426\n",
      "Epoch 5/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - accuracy: 0.7792 - loss: 0.4987 - val_accuracy: 0.8220 - val_loss: 0.4235\n",
      "Epoch 6/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 819us/step - accuracy: 0.7787 - loss: 0.4801 - val_accuracy: 0.8220 - val_loss: 0.4132\n",
      "Epoch 7/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - accuracy: 0.8199 - loss: 0.4265 - val_accuracy: 0.8220 - val_loss: 0.4065\n",
      "Epoch 8/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 803us/step - accuracy: 0.7850 - loss: 0.4592 - val_accuracy: 0.8220 - val_loss: 0.4034\n",
      "Epoch 9/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - accuracy: 0.7753 - loss: 0.4534 - val_accuracy: 0.8220 - val_loss: 0.4022\n",
      "Epoch 10/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 794us/step - accuracy: 0.8187 - loss: 0.4288 - val_accuracy: 0.8220 - val_loss: 0.4015\n",
      "Epoch 11/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 800us/step - accuracy: 0.7884 - loss: 0.4570 - val_accuracy: 0.8220 - val_loss: 0.4014\n",
      "Epoch 12/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 875us/step - accuracy: 0.8196 - loss: 0.4090 - val_accuracy: 0.8220 - val_loss: 0.4008\n",
      "Epoch 13/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.8036 - loss: 0.4285 - val_accuracy: 0.8220 - val_loss: 0.4009\n",
      "Epoch 14/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - accuracy: 0.7690 - loss: 0.4720 - val_accuracy: 0.8220 - val_loss: 0.4015\n",
      "Epoch 15/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 812us/step - accuracy: 0.8012 - loss: 0.4333 - val_accuracy: 0.8220 - val_loss: 0.4022\n",
      "Epoch 16/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - accuracy: 0.7774 - loss: 0.4506 - val_accuracy: 0.8220 - val_loss: 0.4029\n",
      "Epoch 17/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - accuracy: 0.7971 - loss: 0.4156 - val_accuracy: 0.8220 - val_loss: 0.4031\n",
      "Epoch 18/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - accuracy: 0.8033 - loss: 0.4371 - val_accuracy: 0.8220 - val_loss: 0.4032\n",
      "Epoch 19/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - accuracy: 0.7932 - loss: 0.4415 - val_accuracy: 0.8220 - val_loss: 0.4026\n",
      "Epoch 20/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - accuracy: 0.7844 - loss: 0.4280 - val_accuracy: 0.8220 - val_loss: 0.4029\n",
      "Epoch 21/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - accuracy: 0.7997 - loss: 0.4332 - val_accuracy: 0.8136 - val_loss: 0.4023\n",
      "Epoch 22/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - accuracy: 0.7920 - loss: 0.4163 - val_accuracy: 0.8220 - val_loss: 0.4036\n",
      "Epoch 23/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 792us/step - accuracy: 0.7763 - loss: 0.4544 - val_accuracy: 0.8220 - val_loss: 0.4048\n",
      "Epoch 24/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 778us/step - accuracy: 0.7748 - loss: 0.4291 - val_accuracy: 0.8220 - val_loss: 0.4047\n",
      "Epoch 25/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - accuracy: 0.7780 - loss: 0.4304 - val_accuracy: 0.8220 - val_loss: 0.4040\n",
      "Epoch 26/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - accuracy: 0.7788 - loss: 0.4487 - val_accuracy: 0.8136 - val_loss: 0.4038\n",
      "Epoch 27/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - accuracy: 0.7930 - loss: 0.4272 - val_accuracy: 0.8220 - val_loss: 0.4029\n",
      "Epoch 28/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - accuracy: 0.8149 - loss: 0.3923 - val_accuracy: 0.8136 - val_loss: 0.4024\n",
      "Epoch 29/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - accuracy: 0.7602 - loss: 0.4391 - val_accuracy: 0.8220 - val_loss: 0.4024\n",
      "Epoch 30/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - accuracy: 0.7661 - loss: 0.4599 - val_accuracy: 0.8220 - val_loss: 0.4017\n",
      "Epoch 31/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - accuracy: 0.8214 - loss: 0.3856 - val_accuracy: 0.8136 - val_loss: 0.4010\n",
      "Epoch 32/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - accuracy: 0.7799 - loss: 0.4359 - val_accuracy: 0.8220 - val_loss: 0.4006\n",
      "Epoch 33/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step - accuracy: 0.8010 - loss: 0.3894 - val_accuracy: 0.8220 - val_loss: 0.4011\n",
      "Epoch 34/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - accuracy: 0.7940 - loss: 0.4243 - val_accuracy: 0.8136 - val_loss: 0.4002\n",
      "Epoch 35/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 765us/step - accuracy: 0.8348 - loss: 0.3714 - val_accuracy: 0.8220 - val_loss: 0.3995\n",
      "Epoch 36/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - accuracy: 0.7730 - loss: 0.4457 - val_accuracy: 0.8220 - val_loss: 0.3992\n",
      "Epoch 37/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - accuracy: 0.7959 - loss: 0.4117 - val_accuracy: 0.8136 - val_loss: 0.3997\n",
      "Epoch 38/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step - accuracy: 0.7711 - loss: 0.4296 - val_accuracy: 0.8136 - val_loss: 0.3995\n",
      "Epoch 39/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - accuracy: 0.8000 - loss: 0.4087 - val_accuracy: 0.8220 - val_loss: 0.3985\n",
      "Epoch 40/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - accuracy: 0.7852 - loss: 0.4392 - val_accuracy: 0.8220 - val_loss: 0.3974\n",
      "Epoch 41/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - accuracy: 0.8214 - loss: 0.3885 - val_accuracy: 0.8220 - val_loss: 0.3978\n",
      "Epoch 42/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - accuracy: 0.8112 - loss: 0.3907 - val_accuracy: 0.8220 - val_loss: 0.3972\n",
      "Epoch 43/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784us/step - accuracy: 0.7826 - loss: 0.4495 - val_accuracy: 0.8220 - val_loss: 0.3968\n",
      "Epoch 44/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 802us/step - accuracy: 0.7747 - loss: 0.4155 - val_accuracy: 0.8220 - val_loss: 0.3950\n",
      "Epoch 45/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 775us/step - accuracy: 0.7967 - loss: 0.4107 - val_accuracy: 0.8220 - val_loss: 0.3951\n",
      "Epoch 46/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - accuracy: 0.8120 - loss: 0.3892 - val_accuracy: 0.8220 - val_loss: 0.3949\n",
      "Epoch 47/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833us/step - accuracy: 0.8234 - loss: 0.3916 - val_accuracy: 0.8220 - val_loss: 0.3941\n",
      "Epoch 48/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 774us/step - accuracy: 0.7654 - loss: 0.4566 - val_accuracy: 0.8220 - val_loss: 0.3943\n",
      "Epoch 49/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 779us/step - accuracy: 0.8215 - loss: 0.3889 - val_accuracy: 0.8220 - val_loss: 0.3943\n",
      "Epoch 50/50\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 789us/step - accuracy: 0.7998 - loss: 0.4186 - val_accuracy: 0.8220 - val_loss: 0.3935\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - accuracy: 0.7892 - loss: 0.4303\n",
      "Test Accuracy: 82.20%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "drowsy_data = pd.read_csv('drowsy_features.csv')\n",
    "normal_data = pd.read_csv('normal_features.csv')\n",
    "data = pd.concat([drowsy_data, normal_data])\n",
    "\n",
    "X = data[['EAR', 'MAR']].values\n",
    "y = data['Label'].values\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a lightweight neural network\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(2,), activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp141q7e9j/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp141q7e9j/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp141q7e9j'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 2), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  11989586704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12040144656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12034997712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12040144848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12040144464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12040145424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model saved as drowsiness_model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1731589560.727897  110793 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1731589560.727914  110793 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-11-14 18:36:00.728148: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp141q7e9j\n",
      "2024-11-14 18:36:00.728474: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-11-14 18:36:00.728479: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp141q7e9j\n",
      "I0000 00:00:1731589560.731289  110793 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2024-11-14 18:36:00.731802: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-11-14 18:36:00.750417: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/tmp141q7e9j\n",
      "2024-11-14 18:36:00.756115: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 27969 microseconds.\n",
      "2024-11-14 18:36:00.762228: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open('drowsiness_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Model saved as drowsiness_model.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tflite-runtime (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tflite-runtime\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (13.7.0)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-lite (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-lite\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install tflite-runtime\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow-lite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# la\n",
    "from scipy.spatial import distance\n",
    "import dlib\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to calculate Mouth Aspect Ratio (MAR)\n",
    "def calculate_mar(mouth):\n",
    "    A = distance.euclidean(mouth[3], mouth[9])\n",
    "    B = distance.euclidean(mouth[2], mouth[10])\n",
    "    C = distance.euclidean(mouth[4], mouth[8])\n",
    "    D = distance.euclidean(mouth[0], mouth[6])\n",
    "    mar = (A + B + C) / (3.0 * D)\n",
    "    return mar\n",
    "\n",
    "# Load the facial landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download required\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "/var/folders/5d/cxg576h92j906wv9p0xyjc3c0000gn/T/ipykernel_3590/3545488090.py:70: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  prediction = int(output_data[0] > 0.55)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# la\n",
    "from scipy.spatial import distance\n",
    "import dlib\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def calculate_ear(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to calculate Mouth Aspect Ratio (MAR)\n",
    "def calculate_mar(mouth):\n",
    "    A = distance.euclidean(mouth[3], mouth[9])\n",
    "    B = distance.euclidean(mouth[2], mouth[10])\n",
    "    C = distance.euclidean(mouth[4], mouth[8])\n",
    "    D = distance.euclidean(mouth[0], mouth[6])\n",
    "    mar = (A + B + C) / (3.0 * D)\n",
    "    return mar\n",
    "\n",
    "# Load the facial landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")  # Download required\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import lite as tflite\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tflite.Interpreter(model_path='drowsiness_model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Real-time detection\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        left_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(36, 42)])\n",
    "        right_eye = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(42, 48)])\n",
    "        mouth = np.array([(landmarks.part(i).x, landmarks.part(i).y) for i in range(48, 68)])\n",
    "\n",
    "        ear = (calculate_ear(left_eye) + calculate_ear(right_eye)) / 2.0\n",
    "        mar = calculate_mar(mouth)\n",
    "\n",
    "        # Prepare input for the model\n",
    "        input_data = np.array([[ear, mar]], dtype=np.float32)\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "        # Perform inference\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        prediction = int(output_data[0] > 0.55)\n",
    "\n",
    "        # Set rectangle color\n",
    "        color = (0, 0, 255) if prediction == 1 else (0, 255, 0)\n",
    "# color the rectange red if prediction is 1 and green if prediction is 0\n",
    "        # Draw rectangle\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "    cv2.imshow(\"Drowsiness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
